# VL-AnoDiff: Vision-Language Guided Diffusion for Few-Shot Industrial Anomaly Synthesis

Welcome to the official repository for **VL-AnoDiff**, a novel approach for few-shot industrial anomaly synthesis using vision-language guided diffusion models.

## Overview

This repository contains the implementation of VL-AnoDiff, which leverages vision-language models (VLMs) to guide diffusion processes for synthesizing industrial anomalies with limited training data.

## Repository Status

We are actively working on releasing the complete codebase. The current release status is as follows:

- [x] Repository setup
- [ ] VLM-related code
- [ ] Model training code
- [ ] Model inference code
- [ ] Pre-trained weights

## Citation

If you find this work useful in your research, please cite:

```bibtex
@article{vlanodiff2024,
  title={VL-AnoDiff: Vision-Language Guided Diffusion for Few-Shot Industrial Anomaly Synthesis},
  author={Your Name and Collaborators},
  journal={Journal/Conference Name},
  year={2024}
}
```

## License

[To be determined]

## Contact

For questions and inquiries, please open an issue or contact the authors.

